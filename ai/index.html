<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Ais | 我的博客</title>
<meta name="keywords" content="">
<meta name="description" content="Ais - 我的博客">
<meta name="author" content="">
<link rel="canonical" href="https://eightyoliveira.github.io/ai/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.8fe10233a706bc87f2e08b3cf97b8bd4c0a80f10675a143675d59212121037c0.css" integrity="sha256-j&#43;ECM6cGvIfy4Is8&#43;XuL1MCoDxBnWhQ2ddWSEhIQN8A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://eightyoliveira.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://eightyoliveira.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://eightyoliveira.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://eightyoliveira.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://eightyoliveira.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" type="application/rss+xml" href="https://eightyoliveira.github.io/ai/index.xml">
<link rel="alternate" hreflang="en" href="https://eightyoliveira.github.io/ai/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:url" content="https://eightyoliveira.github.io/ai/">
  <meta property="og:site_name" content="我的博客">
  <meta property="og:title" content="Ais">
  <meta property="og:locale" content="zh-cn">
  <meta property="og:type" content="website">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Ais">
<meta name="twitter:description" content="">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Ais",
      "item": "https://eightyoliveira.github.io/ai/"
    }
  ]
}
</script>
</head>

<body class="list" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://eightyoliveira.github.io/" accesskey="h" title="我的博客 (Alt + H)">我的博客</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://eightyoliveira.github.io/java" title="Java">
                    <span>Java</span>
                </a>
            </li>
            <li>
                <a href="https://eightyoliveira.github.io/git" title="git">
                    <span>git</span>
                </a>
            </li>
            <li>
                <a href="https://eightyoliveira.github.io/web" title="web">
                    <span>web</span>
                </a>
            </li>
            <li>
                <a href="https://eightyoliveira.github.io/ai" title="ai">
                    <span class="active">ai</span>
                </a>
            </li>
            <li>
                <a href="https://eightyoliveira.github.io/database" title="database">
                    <span>database</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main"> 
<header class="page-header">
  <h1>
    Ais
  </h1>
</header>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">数据预处理与特征工程
    </h2>
  </header>
  <div class="entry-content">
    <p>Missing Value 进行填补或删除，常用填补方法（均值，0，中位数），也可用算法类随机森林进行填补 常数填补：sklearn.impute类进行填补,特征必须多维，常数填补可以直接用pandas.fillna填补 一般不用算法进行填补，若用随机森林进行填补，先从缺失最少的特征开始，其余特征先填补成0，非最少缺失值列视为y,其余视为x,缺失y对应部分视为测试集 Outlier 箱线图，pd.describe(np.arange(0.1,1.1,0.1)),看最大值，最小值，查看数据的偏态峰态 3sigma原则，删去3sigma外的值 Normalnazation from sklearn.preprocessing import MinMaxScaler scaler = MinMaxScaler() result = scaler.fit_transform(data) scaler.inverse_transform(result) result $ x^* = \frac{x - \min(x)}{\max(x) - \min(x)} $
Standardization from sklearn.preprocessing import StandardScaler scaler = StandardScaler() result = scaler.fit_transform(data) scaler.inverse_transform(result) result $ x^* = \frac{x - \mu}{\sigma} $
Decimal Scaling (十进制缩放) 方法一：通过除以10的幂进行缩放 公式: $ x_i’ = \frac{x_i}{10^j} $
条件:
选择最小的 $ j $，使得缩放后的向量 $ \mathbf{v}’ $ 的最大值小于1。 解释:
该方法通过找到合适的 $ j $ 值，将原始数据 $ x_i $ 缩放到一个更小的范围内，使得所有数据点的最大值不超过1。这有助于保持数据的比例关系，同时缩小数值范围。 方法二：通过对数变换 公式: $ x_i’ = \log(x_i) $
...</p>
  </div>
  <footer class="entry-footer"><span title='2025-05-08 00:00:00 +0000 UTC'>May 8, 2025</span></footer>
  <a class="entry-link" aria-label="post link to 数据预处理与特征工程" href="https://eightyoliveira.github.io/ai/preprocessandfeatuesenginer/"></a>
</article>

<article class="post-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">BERT微调GLUE MRPC数据集指南
    </h2>
  </header>
  <div class="entry-content">
    <p>BERT微调GLUE MRPC数据集指南 1 env env:python=3.6 tensorflow=1.15.0
model:google-bert uncased_L-12_H-768_A-12
2 start_script python run_classifier.py --task_name=MRPC --do_train=true --do_eval=true --data_dir=..\bert\GLUE\glue_data\MRPC --vocab_file=..\bert\GLUE\uncased_L-12_H-768_A-12\vocab.txt --bert_config_file=..\bert\GLUE\uncased_L-12_H-768_A-12\bert_config.json --init_checkpoint=..\bert\GLUE\uncased_L-12_H-768_A-12\bert_model.ckpt --max_seq_length=128 --train_batch_size=8 --learning_rate=2e-5 --num_train_epochs=3.0 --output_dir=..\bert\GLUE\mrpc_output 3 code_analysis 3.1 pre_data num_warmup_steps 是深度学习训练过程中一个非常重要的超参数， 尤其是在使用像 BERT 这样的预训练模型时。 它的作用是定义 学习率预热（learning rate warmup） 的步数，即在训练初期逐渐增加学习率的过程。 为什么需要学习率预热？ 1防止梯度爆炸或不稳定 在训练初期，模型的权重通常是随机初始化的（或者从预训练模型加载的权重与当前任务不完全匹配），此时如果直接使用较大的学习率，可能会导致梯度更新过大，从而引发训练不稳定甚至梯度爆炸。 通过逐步增加学习率，可以让模型在训练初期更平稳地适应数据分布。 2提高收敛速度 学习率预热可以帮助模型更快地找到合适的优化方向，尤其是在使用大批量（large batch size）训练时。 如果直接使用较高的学习率，可能会跳过最优解；而通过预热，可以让模型逐渐逼近最优解。 3BERT 模型的特殊性 BERT 是一个非常大的模型，具有大量的参数。在微调（fine-tuning）阶段，如果直接使用较大的学习率，可能会破坏预训练模型中已经学到的知识（即“灾难性遗忘”问题）。 因此，学习率预热通常被用作一种保护机制，确保模型在微调初期能够稳定地调整权重。 train_examples = None num_train_steps = None num_warmup_steps = None if FLAGS.do_train: train_examples = processor.get_train_examples(FLAGS.data_dir) num_train_steps = int( len(train_examples) / FLAGS.train_batch_size * FLAGS.num_train_epochs) num_warmup_steps = int(num_train_steps * FLAGS.warmup_proportion) #input_ids通过将分词器分开的词，通过vocab_file替换为对应的id features[&#34;input_ids&#34;] = create_int_feature(feature.input_ids) #input_mask标记哪些是有效的token，哪些是padding #features[&#34;input_mask&#34;] = create_int_feature(feature.input_mask) 用于区分输入中的不同句子或段落 features[&#34;segment_ids&#34;] = create_int_feature(feature.segment_ids) #0或者1 features[&#34;label_ids&#34;] = create_int_feature([feature.label_id]) 3.2 create_model 3.2.1 embedding token embedding层 [8,128] -&gt; [8,128,768] [batch_size, seq_length] shape to [batch_size, seq_length, embedding_size]. 通过直接读取预训练好的embedding table，根据单个token转换为768维向量 segment embedding层 输出是[batch_size, seq_length] 二维的embedding，输出是[8,128] position embedding 同上 3.2.2 create_mask 注意！这里的mask是padding mask，而不是transformer论文中的causal Mask def create_attention_mask_from_input_mask(from_tensor, to_mask): &#34;&#34;&#34;Create 3D attention mask from a 2D tensor mask. Args: from_tensor: 2D or 3D Tensor of shape [batch_size, from_seq_length, ...]. to_mask: int32 Tensor of shape [batch_size, to_seq_length]. Returns: float Tensor of shape [batch_size, from_seq_length, to_seq_length]. &#34;&#34;&#34; from_shape = get_shape_list(from_tensor, expected_rank=[2, 3]) batch_size = from_shape[0] 8 from_seq_length = from_shape[1] 128 to_shape = get_shape_list(to_mask, expected_rank=2) to_seq_length = to_shape[1] 8 to_mask = tf.cast( tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32) [8, 1, 128] # We don&#39;t assume that `from_tensor` is a mask (although it could be). We # don&#39;t actually care if we attend *from* padding tokens (only *to* padding) # tokens so we create a tensor of all ones. # # `broadcast_ones` = [batch_size, from_seq_length, 1] broadcast_ones = tf.ones( shape=[batch_size, from_seq_length, 1], dtype=tf.float32) # Here we broadcast along two dimensions to create the mask. mask = broadcast_ones * to_mask [8, 128, 128] return mask to_mask = [ [1, 1, 1, 0], # 第一个样本：前 3 个位置有效，第 4 个位置是填充 [1, 1, 0, 0] # 第二个样本：前 2 个位置有效，后 2 个位置是填充 ] mask = [ [ [1, 1, 1, 0], # 第一个样本，第一个位置可以关注前 3 个位置 [1, 1, 1, 0], # 第一个样本，第二个位置可以关注前 3 个位置 [1, 1, 1, 0], # 第一个样本，第三个位置可以关注前 3 个位置 [0, 0, 0, 0] # 第一个样本，第四个位置是填充，不能关注任何位置 ], [ [1, 1, 0, 0], # 第二个样本，第一个位置可以关注前 2 个位置 [1, 1, 0, 0], # 第二个样本，第二个位置可以关注前 2 个位置 [0, 0, 0, 0], # 第二个样本，第三个位置是填充，不能关注任何位置 [0, 0, 0, 0] # 第二个样本，第四个位置是填充，不能关注任何位置 ] ] 3.2.3 qkv &#34;&#34;&#34;Multi-headed, multi-layer Transformer from &#34;Attention is All You Need&#34;. This is almost an exact implementation of the original Transformer encoder. See the original paper: https://arxiv.org/abs/1706.03762 Also see: https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py Args: [8,128,768] 实际参数 input_tensor: float Tensor of shape [batch_size, seq_length, hidden_size]. [8,128,128] attention_mask: (optional) int32 Tensor of shape [batch_size, seq_length, seq_length], with 1 for positions that can be attended to and 0 in positions that should not be. 768 hidden_size: int. Hidden size of the Transformer. default 12 多层self_attention num_hidden_layers: int. Number of layers (blocks) in the Transformer. default 12 num_attention_heads: int. Number of attention heads in the Transformer. intermediate_size: int. The size of the &#34;intermediate&#34; (a.k.a., feed forward) layer. intermediate_act_fn: function. The non-linear activation function to apply to the output of the intermediate/feed-forward layer. hidden_dropout_prob: float. Dropout probability for the hidden layers. attention_probs_dropout_prob: float. Dropout probability of the attention probabilities. initializer_range: float. Range of the initializer (stddev of truncated normal). do_return_all_layers: Whether to also return all layers or just the final layer. Returns: float Tensor of shape [batch_size, seq_length, hidden_size], the final hidden layer of the Transformer. Raises: ValueError: A Tensor shape or parameter is invalid. &#34;&#34;&#34; 12的倍数，所以768，每个头64个特征 if hidden_size % num_attention_heads != 0: raise ValueError( &#34;The hidden size (%d) is not a multiple of the number of attention &#34; &#34;heads (%d)&#34; % (hidden_size, num_attention_heads)) attention_head_size = int(hidden_size / num_attention_heads) input_shape = get_shape_list(input_tensor, expected_rank=3) batch_size = input_shape[0] seq_length = input_shape[1] input_width = input_shape[2] # The Transformer performs sum residuals on all layers so the input needs # to be the same as the hidden size. if input_width != hidden_size: raise ValueError(&#34;The width of the input tensor (%d) != hidden size (%d)&#34; % (input_width, hidden_size)) # We keep the representation as a 2D tensor to avoid re-shaping it back and # forth from a 3D tensor to a 2D tensor. Re-shapes are normally free on # the GPU/CPU but may not be free on the TPU, so we want to minimize them to # help the optimizer. prev_output = reshape_to_matrix(input_tensor) [1024,768] # Scalar dimensions referenced here: # B = batch size (number of sequences) 8 # F = `from_tensor` sequence length 128 # T = `to_tensor` sequence length 128 # N = `num_attention_heads` 12 # H = `size_per_head` 64 对于每个词，都构建出对应特征向量，12个头，每个头64特征 # `query_layer` = [B*F, N*H] [1024,768] # `key_layer` = [B*T, N*H] # `value_layer` = [B*T, N*H] 在softmax之前，scores算出来之前，将后面的遮蔽 if attention_mask is not None: # `attention_mask` = [B, 1, F, T] attention_mask = tf.expand_dims(attention_mask, axis=[1]) # Since attention_mask is 1.0 for positions we want to attend and 0.0 for # masked positions, this operation will create a tensor which is 0.0 for # positions we want to attend and -10000.0 for masked positions. adder = (1.0 - tf.cast(attention_mask, tf.float32)) * -10000.0 3.2.4 feed forward &amp; dropout &amp; layer normalize
...</p>
  </div>
  <footer class="entry-footer"><span title='2025-04-19 00:00:00 +0000 UTC'>April 19, 2025</span></footer>
  <a class="entry-link" aria-label="post link to BERT微调GLUE MRPC数据集指南" href="https://eightyoliveira.github.io/ai/bertfine-tuningglue-mrpc-data/"></a>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://eightyoliveira.github.io/">我的博客</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
